# [ICCV 25] CWNet: Causal Wavelet Network for Low-Light Image Enhancement :bulb: [Paper](https://arxiv.org/abs/2507.10689) 

#### Tongshun Zhang, Pingping Liu, Yubing Lu, Mengen Cai, Zijian Zhang, Zhe Zhang, Qiuzhan Zhou
#### College of Computer Science and Technology, Jilin University

## 1. Abstract
Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on uniform brightness adjustment, often neglecting instance-level semantic information and the inherent characteristics of different features. To address these limitations, we propose CWNet (Causal Wavelet Network), a novel architecture that leverages wavelet transforms for causal reasoning. Specifically, our approach comprises two key components: 1) Inspired by the concept of intervention in causality, we adopt a causal reasoning perspective to reveal the underlying causal relationships in low-light enhancement. From a global perspective, we employ a metric learning strategy to ensure causal embeddings adhere to causal principles, separating them from non-causal confounding factors while focusing on the invariance of causal factors. At the local level, we introduce an instance-level CLIP semantic loss to precisely maintain causal factor consistency. 2) Based on our causal analysis, we present a wavelet transform-based backbone network that  effectively  optimizes the recovery of frequency information, ensuring precise enhancement tailored to the specific attributes of wavelet transforms. Extensive experiments demonstrate that CWNet significantly outperforms current state-of-the-art methods across multiple datasets, showcasing its robust performance across diverse scenes.

## CWNet Overall Architecture

![Over-all-Architecture](https://github.com/user-attachments/assets/2fea117c-a23e-44b5-b731-b19f931716a6)

## Performance

![Performance](https://github.com/user-attachments/assets/ef8b22c4-de69-4b70-a352-4f9951ac1798)

![Performance](https://github.com/user-attachments/assets/1d6a3968-60f4-441d-8e9a-bb80249f1b9a)

![Performance](https://github.com/user-attachments/assets/2d9b293c-3083-43fe-b90b-ea520da72fef)


## 2. Prepare Dataset

LOL-v1 [Baidu Disk](https://pan.baidu.com/share/init?surl=ZAC9TWR-YeuLIkWs3L7z4g&pwd=cyh2) (code: cyh2), [Google Drive](https://drive.google.com/file/d/1L-kqSQyrmMueBh_ziWoPFhfsAh50h20H/view?usp=sharing)

LOL-v2 [Baidu Disk](https://pan.baidu.com/share/init?surl=ZAC9TWR-YeuLIkWs3L7z4g&pwd=cyh2) (code: cyh2), [Google Drive](https://drive.google.com/file/d/1Ou9EljYZW8o5dbDCf9R34FS8Pd8kEp2U/view?usp=sharing)

LSRW-Huawei [Baidu Disk](https://pan.baidu.com/s/1XHWQAS0ZNrnCyZ-bq7MKvA) (code:wmrr)

## 3. Create Environment

### Installation
```
conda env create -f environment.yml
conda activate cwnet
```
(1) First download the pre-trained model (DA-CLIP, Segmentation model) from [Google Drive](https://drive.google.com/drive/folders/1Bcom7bANqh1_m2rNgEuG7C_JAAAF1bEh?usp=sharing).

(2) Place the DA-CLIP pre-trained model daclip_ViT-B-32.pt in the models/archs/clip directory.

(3) Download the pre-trained segmentation model hrnet_w48_pascal_context_cls59_480x480.pth, and configure line 21 of models/archs/segment/hrseg_model.py: pretrained_dict.

(4) Configure the path in /options/xx.yml:

lightmap_GT: (Generate the light map path via dataroot_GT, generated by LIME (LIME: Low-Light Image Enhancement via Illumination Map Estimation). It can also be obtained from [Google Drive](https://drive.google.com/drive/folders/1Bcom7bANqh1_m2rNgEuG7C_JAAAF1bEh?usp=sharing).)
dataroot_GT:  your image path
dataroot_LQ:  your image path

### Train
```
python train.py -opt ./options/train/huawei.yml
```

### Test
```
python test.py -opt ./options/test/huawei.yml
```

### Pre-trained Models and Outputs
âœ¨ [Google Drive](https://drive.google.com/drive/folders/1Bcom7bANqh1_m2rNgEuG7C_JAAAF1bEh?usp=sharing).


## Citation Information
If you find the project useful, please cite:  

```bibtex  
@misc{zhang2025cwnetcausalwaveletnetwork,
      title={CWNet: Causal Wavelet Network for Low-Light Image Enhancement}, 
      author={Tongshun Zhang and Pingping Liu and Yubing Lu and Mengen Cai and Zijian Zhang and Zhe Zhang and Qiuzhan Zhou},
      year={2025},
      eprint={2507.10689},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.10689}, 
}

